{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a79ec0be",
   "metadata": {},
   "source": [
    "## **Atividade 2 — Aprendizado não Supervisionado**\n",
    "\n",
    "Nesta atividade, você deve utilizar **métodos de aprendizado não supervisionado** vistos em sala de aula, como **k-means** ou **redes SOM**, para **identificar padrões nos dados de engajamento**.\n",
    "\n",
    "O objetivo é compreender **quais tópicos, padrões ou grupos de postagens** estão mais associados a alto ou baixo engajamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6198feef",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gdown 19Y-NdpOTCfmtlM66FqodAtiZMs8lGQTq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8b796c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "df = pd.read_pickle('df_social_data_train.pkl')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840829be",
   "metadata": {},
   "source": [
    "## Pré Processando os Dados e Classificando-os em clusters com K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982389e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-processar e extrair caracteristicas dos dados para construir a tabela atributo-valor\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# 1. Load a pretrained Sentence Transformer model\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8312a46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['features'] = list(model.encode(df['content'].tolist(), show_progress_bar=True))\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4659dc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['content','features']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3638d841",
   "metadata": {},
   "source": [
    "Separamos o conjunto de dados entre:\n",
    "- Posts com engajamento low\n",
    "- Posts com engajamento high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a66f8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_low = df[df['engagement'] == 'low']\n",
    "df_high = df[df['engagement'] == 'high']\n",
    "features_low = np.array(df_low.features.to_list())\n",
    "features_high = np.array(df_high.features.to_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c242c5e0",
   "metadata": {},
   "source": [
    "KMeans com seu valor padrão de clusteres: 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c075fd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_low = KMeans(random_state=0, n_init=\"auto\").fit(features_low)\n",
    "kmeans_high = KMeans(random_state=0, n_init=\"auto\").fit(features_high)\n",
    "df_low['cluster'] = kmeans_low.labels_\n",
    "df_high['cluster'] = kmeans_high.labels_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f02a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_low"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce77db8",
   "metadata": {},
   "source": [
    "Plot dos clusteres para visualização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0143656",
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "\n",
    "y_low = np.array(df_low.cluster.to_list())\n",
    "y_high = np.array(df_high.cluster.to_list())\n",
    "\n",
    "features2D_low = umap.UMAP().fit_transform(features_low, y=y_low)\n",
    "features2D_high = umap.UMAP().fit_transform(features_high, y=y_high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24697a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style='white', context='poster')\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))  # 1 row, 2 columns\n",
    "\n",
    "# Plot for low cluster data on the first subplot\n",
    "axes[0].scatter(features2D_low[:, 0], features2D_low[:, 1], s=0.1,\n",
    "                c=df_low.cluster.to_list(), cmap='Spectral', alpha=1.0)\n",
    "axes[0].set_title('Low Cluster')\n",
    "\n",
    "# Plot for high cluster data on the second subplot\n",
    "axes[1].scatter(features2D_high[:, 0], features2D_high[:, 1], s=0.1,\n",
    "                c=df_high.cluster.to_list(), cmap='Spectral', alpha=1.0)\n",
    "axes[1].set_title('High Cluster')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0069bcf5",
   "metadata": {},
   "source": [
    "## Instanciação de LLM Ollama 3.2\n",
    "\n",
    "O reconhecimentos dos temas comuns entre os clusteres será feito utilizando uma LLM. A IA generativa deverá inferir:\n",
    "- O Tema Geral dos posts agregados em um mesmo cluster\n",
    "- Palavras chaves que descrevem esse cluster\n",
    "- Quais as características de posts dentro deste cluster\n",
    "- Criar um \"post\" de exemplo que ilustre esse cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e455bd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai==0.28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ffdb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "import openai\n",
    "\n",
    "def llm_local():\n",
    "  openai.api_base = \"http://localhost:11434/v1\"\n",
    "  openai.api_key = \"ollama\"  # Requerido por algumas bibliotecas mesmo que não usado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5adf2bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import openai\n",
    "\n",
    "def llm_task(model, system, prompt):\n",
    "  response = openai.ChatCompletion.create(\n",
    "      model=model,\n",
    "      messages = [\n",
    "            {\n",
    "              \"role\": \"system\",\n",
    "              \"content\": system\n",
    "            },\n",
    "            {\n",
    "              \"role\": \"user\",\n",
    "              \"content\": prompt\n",
    "            },\n",
    "          ]\n",
    "\n",
    "  )\n",
    "  s = response['choices'][0]['message']['content'].strip()\n",
    "\n",
    "  return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9549d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -fsSL https://ollama.com/install.sh | sh\n",
    "!pip install ollama\n",
    "!nohup ollama serve &"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52dedb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# baixando uma LLM (llama3.2)\n",
    "!ollama pull llama3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c88ba66",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_local()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5afed798",
   "metadata": {},
   "source": [
    "## Systems Roles\n",
    "\n",
    "Para as chamadas à LLM Ollama, haverão três sistemas para obter as informações desejadas. O Primeiro terá o papel de identificar as características mencionadas anteriormete. O Segundo será responsável por estruturar a resposta do primeiro em um formato JSON. O terceiro será usado para verificar se a resposta do segundo está no formato correto de JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662d156b",
   "metadata": {},
   "outputs": [],
   "source": [
    "system = '''\n",
    "You are a social media post analyst.\n",
    "Given a list of posts, you must:\n",
    "1) Identify ONLY ONE general theme of the posts. DO NOT GIVE ME MORE THAN ONE THEME.\n",
    "2) Identify the most relevant topics in the posts. Each topic should consist of 2 or 3 words.\n",
    "3) Identify characteristics of how those posts are written, if it is personal or not, if it uses emojis, etc.\n",
    "4) Summarize the posts into a single one\n",
    "\n",
    "Structure your response as follows:\n",
    "  - General Theme: A text that clearly summarizes the general theme of the received posts.\n",
    "  - Relevant Topics: A list of relevant topics you identified in the received posts.\n",
    "  - Characteristics: A text that clearly summarizes the characteristics of the posts.\n",
    "  - Summary of posts: A text that summarizes the posts.\n",
    "\n",
    "Write the output in Portuguese and ONLY in the structure I provided.\n",
    "Do not return anything beyond that.\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25422842",
   "metadata": {},
   "outputs": [],
   "source": [
    "system2 = '''\n",
    "Você é um formatador de análise de posts em redes sociais. Você será utilizado para formatar uma resposta anterior em um JSON.\n",
    "Você receperá um texto em inglês, interprete-o e depois formate em um JSON em português.\n",
    "A saída da análise deve ser estruturada em JSON.\n",
    "\n",
    "Exemplo de Estrutura do JSON:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"tema_geral\": \"texto do tema geral\",\n",
    "  \"topicos_relevantes\": [\"topico A\",\"topico B\",\"topico C\"],\n",
    "  \"caracteristicas\" : \"caracteristicas do post\",\n",
    "  \"resumo_posts\": \"resumir posts\",\n",
    "}\n",
    "```\n",
    "\n",
    "A resposta deve ser em português e APENAS EM JSON.\n",
    "Não retorne nada além do JSON. Não precisa me responder, apenas envie o JSON.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64e8da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "system3 = '''\n",
    "Você é uma ferramenta que corrige eventuais erros de JSON.\n",
    "SE ESTIVER CORRETO APENAS ME RETORNE O JSON.\n",
    "\n",
    "Exemplo de Estrutura do JSON correto:\n",
    "\n",
    "\n",
    "{\n",
    "  \"tema_geral\": \"texto do tema geral\",\n",
    "  \"topicos_relevantes\": [\"topico A\",\"topico B\",\"topico C\"],\n",
    "  \"caracteristicas\" : \"caracteristicas do post\",\n",
    "  \"resumo_posts\": \"resumir posts\",\n",
    "}\n",
    "\n",
    "A resposta deve ser em português e APENAS EM JSON.\n",
    "Não retorne nada além do JSON. Não precisa me responder, apenas envie o JSON.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbd9641",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_low.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d90a3b5",
   "metadata": {},
   "source": [
    "chamadas à LLM para identificação das características"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f136fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "L = []\n",
    "low_clusters = {}\n",
    "high_clusters = {}\n",
    "\n",
    "def identify_clusters(df, cluster_dict):\n",
    "  for cluster in range(0,8):\n",
    "    print(\"\\n\\nGerando analise do Cluster\", cluster)\n",
    "    prompt = ''\n",
    "    df_temp = df[df.cluster == cluster].sample(30)\n",
    "    for index,row in df_temp.iterrows():\n",
    "      prompt += f'#Post: {row.content} \\n\\n\\n'\n",
    "    #print(prompt)\n",
    "\n",
    "    resposta = llm_task(\"llama3.2\",system,prompt)\n",
    "    print(resposta)\n",
    "    resposta2 = llm_task(\"llama3.2\",system2,resposta)\n",
    "    json_str = resposta2.replace(\"```json\",\"\").replace(\"```\",\"\")\n",
    "    obj = json.loads(json_str)\n",
    "    print(obj)\n",
    "\n",
    "    df_temp['tema_geral'] = obj['tema_geral']\n",
    "    df_temp['topicos_relevantes'] = str(obj.get('topicos_relevantes', []))\n",
    "    df_temp['caracteristicas'] = str(obj.get('caracteristicas', []))\n",
    "    df_temp['resumo_posts'] = str(obj.get('resumo_posts', []))\n",
    "\n",
    "    L.append(df_temp)\n",
    "    cluster_dict[cluster] = obj\n",
    "    print(\"======\")\n",
    "\n",
    "\n",
    "\n",
    "identify_clusters(df_low, low_clusters)\n",
    "identify_clusters(df_high, high_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9566d808",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clusters = pd.concat(L)\n",
    "df_clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca13a34",
   "metadata": {},
   "source": [
    "## Print final dos Resultados Obtidos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c07973",
   "metadata": {},
   "source": [
    "Low clusteres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2982a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in low_clusters.items():\n",
    "  print(\"Cluster: \", key)\n",
    "  print(\"Tema Geral: \", value['tema_geral'])\n",
    "  print(\"Tópicos Relevantes: \", value['topicos_relevantes'])\n",
    "  try:\n",
    "    print(\"Caracteristicas: \", value['caracteristicas'])\n",
    "  except:\n",
    "    print(\"Caracteristicas: \")\n",
    "  print(\"Resumo Posts: \", value['resumo_posts'])\n",
    "  print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8a0f80",
   "metadata": {},
   "source": [
    "High clusteres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3662a817",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in high_clusters.items():\n",
    "  print(\"Cluster: \", key)\n",
    "  print(\"Tema Geral: \", value['tema_geral'])\n",
    "  print(\"Tópicos Relevantes: \", value['topicos_relevantes'])\n",
    "  try:\n",
    "    print(\"Caracteristicas: \", value['caracteristicas'])\n",
    "  except:\n",
    "    print(\"Caracteristicas: \")\n",
    "  print(\"Resumo Posts: \", value['resumo_posts'])\n",
    "  print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c6019e",
   "metadata": {},
   "source": [
    "## **Atividade 3 — Desenvolvimento de um Agente Criativo para Geração de Postagens Engajadas**\n",
    "\n",
    "Esta é uma atividade **aberta à criatividade dos alunos**. O desafio consiste em desenvolver um **agente inteligente** que utilize **modelos de aprendizado profundo ou inteligência artificial generativa** para **auxiliar na criação de postagens com maior potencial de engajamento**.\n",
    "\n",
    "Esse agente pode explorar:\n",
    "\n",
    "* Análise de características de postagens com alto engajamento\n",
    "* Reescrita ou sugestão de novos textos com base em um post inicial\n",
    "* Combinação de diferentes abordagens estudadas ao longo da disciplina\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78590f87",
   "metadata": {},
   "source": [
    "# Solução:\n",
    "O agente proposto possuirá uma ferramenta de melhora de Posts, visando um aumento no possível engajamento.\n",
    "Para isso, a ferramenta de MelhorarPost fará uma chamada à LLM Ollama, possuindo como contexto, as características identificadas no exercício anterior.\n",
    "A LLM tentará:\n",
    "- Inferir a qual cluster esse Post melhor se assimilaria\n",
    "- Modificar o Post Original com base nas anotações de engajamento 'High' previstas para aquele cluster\n",
    "- Retornar o Post Final com as mudanças efetuadas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec30f6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q langchain langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2de93d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "llm = ChatOllama(model=\"llama3.2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9b475c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testando a LLM\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "resposta = llm([HumanMessage(content=\"Olá LLM\")])\n",
    "print(resposta.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4b0cd4",
   "metadata": {},
   "source": [
    "System que da o contexto do sistema (nesse caso é apenas uma parte do prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92283af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "system4 = '''\n",
    "Você é um modificador de posts em redes sociais.\n",
    "Você receberá um post a ser classificado entre os clusters e melhorado.\n",
    "As suas mudanças deverão ser baseadas nas características dos Clusteres.\n",
    "\n",
    "Os clusteres que você terá disponíveis para ver as características serão:\n",
    "'''\n",
    "\n",
    "for key, value in high_clusters.items():\n",
    "  system4 += f\"#Cluster: {key} \\n\"\n",
    "  system4 += f\"#Tema Geral: {value['tema_geral']} \\n\"\n",
    "  system4 += f\"#Tópicos Relevantes: {value['topicos_relevantes']} \\n\"\n",
    "  try:\n",
    "    system4 += f\"#Caracteristicas: {value['caracteristicas']} \\n\"\n",
    "  except:\n",
    "    continue\n",
    "\n",
    "system4 += '''\n",
    "\n",
    "⚠️ SUA RESPOSTA DEVE CONTER O POST FINAL MELHORADO. NÃO INCLUA ANÁLISES OU PENSAMENTOS, APENAS O POST FINAL.\n",
    "\n",
    "RETORNE APENAS O POST FINAL.\n",
    "RETORNE APENAS O POST FINAL.\n",
    "RETORNE APENAS O POST FINAL.\n",
    "RETORNE APENAS O POST FINAL.\n",
    "RETORNE APENAS O POST FINAL.\n",
    "RETORNE APENAS O POST FINAL.\n",
    "RETORNE APENAS O POST FINAL.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4651eb0",
   "metadata": {},
   "source": [
    "Criação da tool de melhorar post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b01a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import Tool\n",
    "\n",
    "# Função auxiliar para exibir resultados formatados\n",
    "\n",
    "# Função principal da tool\n",
    "def melhorar_post_simples(post: str) -> str:\n",
    "    prompt = system4 + f\"\\nPost: {post}\"\n",
    "    resposta = llm.invoke([HumanMessage(content=prompt)])\n",
    "    return resposta.content.strip()\n",
    "\n",
    "# Criar a tool para LangChain\n",
    "tool_melhorarPost = Tool(\n",
    "    name=\"Melhorar_Post\",\n",
    "    func=melhorar_post_simples,\n",
    "    description=(\n",
    "        \"Melhorar o post de uma rede social.\"\n",
    "        \"Use esta ferramenta quando a pergunta envolver melhorar Posts que serão usados para rede sociais. Retornar apenas o conteúdo do post, nada de comentários\"\n",
    "    ),\n",
    "    return_direct=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61e8716",
   "metadata": {},
   "source": [
    "Criação do agente utilizando a tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92952405",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents.agent_types import AgentType\n",
    "\n",
    "agent = initialize_agent(\n",
    "    tools=[tool_melhorarPost],\n",
    "    llm=llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    handle_parsing_errors=True,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70bb711",
   "metadata": {},
   "source": [
    "Testes para validar a tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bf4e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Pode melhorar esse post para mim? 'Hoje acordei cedo e trabalhei bastante. Produtividade é tudo!'\"\n",
    "response = agent.run(query)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c4ef1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Pode melhorar esse post para mim? 'Às vezes tudo o que precisamos é de um café forte e foco total.'\"\n",
    "response = agent.run(query)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
